{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used in Tensorflow Model\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "\n",
    "#Usde to for Contextualisation and Other NLP Tasks.\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "#Other\n",
    "import json\n",
    "import pickle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the Intents.....\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    print(\"Processing the Intents.....\")\n",
    "    with open('intents.json') as json_data:\n",
    "        intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    words = []\n",
    "    classes = []\n",
    "    documents = []\n",
    "    ignore_words = ['?']\n",
    "    print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n",
    "    for intent in intents['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            # tokenize each word in the sentence\n",
    "            w = nltk.word_tokenize(pattern)\n",
    "            # add to our words list\n",
    "            words.extend(w)\n",
    "            # add to documents in our corpus\n",
    "            documents.append((w, intent['tag']))\n",
    "            # add to our classes list\n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming, Lowering and Removing Duplicates.......\n",
      "27 documents\n",
      "9 classes ['goodbye', 'greeting', 'hours', 'mopeds', 'opentoday', 'payments', 'rental', 'thanks', 'today']\n",
      "48 unique stemmed words [\"'d\", \"'s\", 'a', 'acceiv', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'do', 'doe', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hour', 'how', 'i', 'is', 'kind', 'lat', 'lik', 'mastercard', 'mop', 'of', 'on', 'op', 'rent', 'see', 'tak', 'thank', 'that', 'ther', 'thi', 'to', 'today', 'we', 'what', 'when', 'which', 'work', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    print(\"Stemming, Lowering and Removing Duplicates.......\")\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    # remove duplicates\n",
    "    classes = sorted(list(set(classes)))\n",
    "\n",
    "    print (len(documents), \"documents\")\n",
    "    print (len(classes), \"classes\", classes)\n",
    "    print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Data for our Model.....\n",
      "Creating Traning Set, Bag of Words for our Model....\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    print(\"Creating the Data for our Model.....\")\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    print(\"Creating Traning Set, Bag of Words for our Model....\")\n",
    "    for doc in documents:\n",
    "        # initialize our bag of words\n",
    "        bag = []\n",
    "        # list of tokenized words for the pattern\n",
    "        pattern_words = doc[0]\n",
    "        # stem each word\n",
    "        pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "        # create our bag of words array\n",
    "        for w in words:\n",
    "            bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "        # create a tuple with bag of words and tag regerences.\n",
    "        training.append([bag, doc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling Randomly and Converting into Numpy Array for Faster Processing......\n",
      "Creating Train and Test Lists.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3999/375135728.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training)\n",
    "\n",
    "    print(\"Creating Train and Test Lists.....\")\n",
    "    train_x = list(training[:,0])\n",
    "    train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model'):\n",
    "    model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model.......\n",
      "Saving the Model.......\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    print(\"Training the Model.......\")\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Saving the Model.......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle is also Saved..........\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('model'):\n",
    "    print(\"Pickle is also Saved..........\")\n",
    "    pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, \n",
    "                open( \"training_data\", \"wb\" ) )\n",
    "    pickle.dump(model, open(\"model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pickle.....\n",
      "Loading the Model......\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Pickle.....\")\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "    \n",
    "print(\"Loading the Model......\")\n",
    "# load our saved model\n",
    "model = pickle.load(open(\"model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # It Tokenize or Break it into the constituents parts of Sentense.\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # Stemming means to find the root of the word.\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    result = model.predict([bow(sentence, words)])\n",
    "    if result:\n",
    "        # Long Loop to get the Result.\n",
    "        for i in intents['intents']:\n",
    "            # Tag Finding\n",
    "            if i['tag'] == result:\n",
    "                # Random Response from High Order Probabilities\n",
    "                return random.choice(i['responses'])\n",
    "            \n",
    "    return 'Repeat Please'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there, how can I help?\n",
      "Hi there, how can I help?\n",
      "Hi there, how can I help?\n",
      "Our hours are 9am-9pm every day\n",
      "Our hours are 9am-9pm every day\n",
      "We're open every day from 9am-9pm\n",
      "We're open every day from 9am-9pm\n",
      "Good to see you again\n",
      "Good to see you again\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/elkin/Documents/Proyectos/Chatbot/main_sklearn.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elkin/Documents/Proyectos/Chatbot/main_sklearn.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elkin/Documents/Proyectos/Chatbot/main_sklearn.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     input_data \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mYou- \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elkin/Documents/Proyectos/Chatbot/main_sklearn.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     answer \u001b[39m=\u001b[39m response(input_data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elkin/Documents/Proyectos/Chatbot/main_sklearn.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(answer)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1178\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1181\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1182\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    input_data = input(\"You- \")\n",
    "    answer = response(input_data)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
